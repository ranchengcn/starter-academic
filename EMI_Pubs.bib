@inproceedings{he_surrogate-assisted_2019,
	title = {Surrogate-{Assisted} {Expensive} {Many}-{Objective} {Optimization} by {Model} {Fusion}},
	doi = {10.1109/CEC.2019.8790155},
	abstract = {Surrogate-assisted evolutionary algorithms have played an important role in expensive optimization where a small number of real-objective function evaluations are allowed. Usually, the surrogate models are used for the same purpose, e.g., to approximate the real-objective function or the aggregation fitness function. However, there is little work on surrogate-assisted optimization by model fusion, i.e., different surrogate models are fused for different purposes to improve the performance of the algorithm. In this work, we propose a surrogate-assisted approach by model fusion for solving expensive many-objective optimization problems, in which the Kriging assisted objective function approximation method is fused with the classifier assisted approach. The proposed algorithm is compared with some state-of-the-art surrogate-assisted algorithms on DTLZ problems and a real-world problem, and some encouraging results have been achieved by our proposed model fusion based approach.},
	booktitle = {2019 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {He, Cheng and Cheng, Ran and Jin, Yaochu and Yao, Xin},
	year = {2019},
	keywords = {Evolutionary computation, Optimization, Sociology, Statistics, Approximation algorithms, classification, Computational modeling, Expensive problem, fitness approximation, Kriging, many-objective optimization, model fusion, surrogate-assisted optimization, Uncertainty},
	pages = {1672--1679},
}

@article{he_efficient_2021,
	title = {Efficient evolutionary neural architecture search by modular inheritable crossover},
	volume = {64},
	issn = {2210-6502},
	url = {https://www.sciencedirect.com/science/article/pii/S2210650221000559},
	doi = {10.1016/j.swevo.2021.100894},
	abstract = {Deep neural networks are widely used in the domain of image classification, and a large number of excellent deep neural networks have been proposed in recent years. However, hand-crafted neural networks often require human experts for elaborate designs, which can be time-consuming and error-prone. Hence, neural architecture search (NAS) methods have been proposed to design model architecture automatically. The evolutionary NAS methods have achieved encouraging results due to the global search capability of evolutionary algorithms. Nevertheless, most existing evolutionary NAS methods use only the mutation operator to generate offspring architectures. Consequently, the generated architectures could be pretty different from their parent architectures, failing to inherit the modular information to accelerate the convergence rate. We propose an efficient evolutionary method using a tailored crossover operator to address this deficiency, which enables the offspring architectures to inherit from their parent architectures. Moreover, we combine it with mutation operators under the framework of the evolutionary algorithm. Experimental results on both the CIFAR-10 and CIFAR-100 tasks show that our proposed evolutionary NAS method has achieved state-of-the-art results.},
	language = {en},
	urldate = {2021-08-01},
	journal = {Swarm and Evolutionary Computation},
	author = {He, Cheng and Tan, Hao and Huang, Shihua and Cheng, Ran},
	year = {2021},
	keywords = {Evolutionary algorithm, Image classification, Inheritable crossover, Neural architecture search},
	pages = {100894},
}

@inproceedings{chen_efficient_2020,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Efficient {Evolutionary} {Deep} {Neural} {Architecture} {Search} ({NAS}) by {Noisy} {Network} {Morphism} {Mutation}},
	isbn = {9789811534157},
	doi = {10.1007/978-981-15-3415-7_41},
	abstract = {Deep learning has achieved enormous breakthroughs in the field of image recognition. However, due to the time-consuming and error-prone process in discovering novel neural architecture, it remains a challenge for designing a specific network in handling a particular task. Hence, many automated neural architecture search methods are proposed to find suitable deep neural network architecture for a specific task without human experts. Nevertheless, these methods are still computationally/economically expensive, since they require a vast amount of computing resource and/or computational time. In this paper, we propose several network morphism mutation operators with extra noise, and further redesign the macro-architecture based on the classical network. The proposed methods are embedded in an evolutionary algorithm and tested on CIFAR-10 classification task. Experimental results indicate the capability of our proposed method in discovering powerful neural architecture which has achieved a classification error 2.55\% with only 4.7M parameters on CIFAR-10 within 12 GPU-hours.},
	language = {en},
	booktitle = {Bio-inspired {Computing}: {Theories} and {Applications}},
	publisher = {Springer},
	author = {Chen, Yiming and Pan, Tianci and He, Cheng and Cheng, Ran},
	editor = {Pan, Linqiang and Liang, Jing and Qu, Boyang},
	year = {2020},
	keywords = {Evolutionary algorithm, Neural architecture search, Network morphism},
	pages = {497--508},
}

@inproceedings{he_iterated_2020,
	title = {Iterated {Problem} {Reformulation} for {Evolutionary} {Large}-{Scale} {Multiobjective} {Optimization}},
	doi = {10.1109/CEC48606.2020.9185553},
	abstract = {Due to the curse of dimensionality, two main issues remain challenging for applying evolutionary algorithms (EAs) to large-scale multiobjective optimization. The first issue is how to improve the efficiency of EAs for reducing computation cost. The second one is how to improve the diversity maintenance of EAs to avoid local optima. Nevertheless, these two issues are somehow conflicting with each other, and thus it is crucial to strike a balance between them in practice. Thereby, we propose an iterated problem reformulation based EA for large-scale multiobjective optimization, where the problem reformulation based method and the decomposition based method are used iteratively to address the aforementioned issues. The proposed method is compared with several state-of-the-art EAs on a variety of large-scale multiobjective optimization problems. Experimental results demonstrate the effectiveness of our proposed iterated method in large-scale multiobjective optimization.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {He, Cheng and Cheng, Ran and Tian, Ye and Zhang, Xingyi},
	year = {2020},
	keywords = {Convergence, Sociology, Pareto optimization, Indexes, Computer science, Evolutionary algorithm, large-scale optimization, multiobjective optimization, problem reformulation},
	pages = {1--8},
}

@inproceedings{lin_dimension_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dimension {Dropout} for {Evolutionary} {High}-{Dimensional} {Expensive} {Multiobjective} {Optimization}},
	isbn = {978-3-030-72062-9},
	doi = {10.1007/978-3-030-72062-9_45},
	abstract = {In the past decades, a number of surrogate-assisted evolutionary algorithms (SAEAs) have been developed to solve expensive multiobjective optimization problems (EMOPs). However, most existing SAEAs focus on low-dimensional optimization problems, since a large number of training samples are required (which is unrealistic for EMOPs) to build an accurate surrogate model for high-dimensional problems. In this paper, an SAEA with Dimension Dropout is proposed to solve high-dimensional EMOPs. At each iteration of the proposed algorithm, it randomly selects a part of the decision variables by Dimension Dropout, and then optimizes the selected decision variables with the assistance of surrogate models. To balance the convergence and diversity, those candidate solutions with good diversity are modified by replacing the selected decision variables with those optimized ones (i.e., decision variables from some better-converged candidate solutions). Eventually, the new candidate solutions are evaluated using expensive functions to update the archive. Empirical studies on ten benchmark problems with up to 200 decision variables demonstrate the competitiveness of the proposed algorithm.},
	language = {en},
	booktitle = {Evolutionary {Multi}-{Criterion} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Lin, Jianqing and He, Cheng and Cheng, Ran},
	editor = {Ishibuchi, Hisao and Zhang, Qingfu and Cheng, Ran and Li, Ke and Li, Hui and Wang, Handing and Zhou, Aimin},
	year = {2021},
	keywords = {Multiobjective optimization, Dimension dropout, High-dimensional, Surrogate-assisted optimization},
	pages = {567--579},
}

@article{he_accelerating_2019,
	title = {Accelerating {Large}-{Scale} {Multiobjective} {Optimization} via {Problem} {Reformulation}},
	volume = {23},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2019.2896002},
	abstract = {In this paper, we propose a framework to accelerate the computational efficiency of evolutionary algorithms on large-scale multiobjective optimization. The main idea is to track the Pareto optimal set (PS) directly via problem reformulation. To begin with, the algorithm obtains a set of reference directions in the decision space and associates them with a set of weight variables for locating the PS. Afterwards, the original large-scale multiobjective optimization problem is reformulated into a low-dimensional single-objective optimization problem. In the reformulated problem, the decision space is reconstructed by the weight variables and the objective space is reduced by an indicator function. Thanks to the low dimensionality of the weight variables and reduced objective space, a set of quasi-optimal solutions can be obtained efficiently. Finally, a multiobjective evolutionary algorithm is used to spread the quasi-optimal solutions over the approximate Pareto optimal front evenly. Experiments have been conducted on a variety of large-scale multiobjective problems with up to 5000 decision variables. Four different types of representative algorithms are embedded into the proposed framework and compared with their original versions, respectively. Furthermore, the proposed framework has been compared with two state-of-the-art algorithms for large-scale multiobjective optimization. The experimental results have demonstrated the significant improvement benefited from the framework in terms of its performance and computational efficiency in large-scale multiobjective optimization.},
	number = {6},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {He, Cheng and Li, Lianghao and Tian, Ye and Zhang, Xingyi and Cheng, Ran and Jin, Yaochu and Yao, Xin},
	year = {2019},
	keywords = {Convergence, Evolutionary computation, Pareto optimization, large-scale optimization, multiobjective optimization, problem reformulation, Acceleration, Evolutionary algorithms, IEEE Fellows, Signal processing algorithms},
	pages = {949--961},
	annote = {Conference Name: IEEE Transactions on Evolutionary Computation},
}

@article{he_paired_2021,
	title = {Paired {Offspring} {Generation} for {Constrained} {Large}-{Scale} {Multiobjective} {Optimization}},
	volume = {25},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2020.3047835},
	abstract = {Constrained multiobjective optimization problems (CMOPs) widely exist in real-world applications, and they are challenging for conventional evolutionary algorithms (EAs) due to the existence of multiple constraints and objectives. When the number of objectives or decision variables is scaled up in CMOPs, the performance of EAs may degenerate dramatically and may fail to obtain any feasible solutions. To address this issue, we propose a paired offspring generation-based multiobjective EA for constrained large-scale optimization. The general idea is to emphasize the role of offspring generation in reproducing some promising feasible or useful infeasible offspring solutions. We first adopt a small set of reference vectors for constructing several subpopulations with a fixed number of neighborhood solutions. Then, a pairing strategy is adopted to determine some pairwise parent solutions for offspring generation. Consequently, the pairwise parent solutions, which could be infeasible, may guide the generation of well-converged solutions to cross the infeasible region(s) effectively. The proposed algorithm is evaluated on CMOPs with up to 1000 decision variables and ten objectives. Moreover, each component in the proposed algorithm is examined in terms of its effect on the overall algorithmic performance. Experimental results on a variety of existing and our tailored test problems demonstrate the effectiveness of the proposed algorithm in constrained large-scale multiobjective optimization.},
	number = {3},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {He, Cheng and Cheng, Ran and Tian, Ye and Zhang, Xingyi and Tan, Kay Chen and Jin, Yaochu},
	year = {2021},
	keywords = {Convergence, Optimization, Sociology, Statistics, Pareto optimization, many-objective optimization, large-scale optimization, multiobjective optimization, Signal processing algorithms, Constraint handling, evolutionary algorithm (EA)},
	pages = {448--462},
	annote = {Conference Name: IEEE Transactions on Evolutionary Computation},
}


@inproceedings{li_large-scale_2021,
	title = {Large-scale {Multiobjective} {Optimization} via {Problem} {Decomposition} and {Reformulation}},
	doi = {10.1109/CEC45853.2021.9504820},
	abstract = {Large-scale multiobjective optimization problems (LSMOPs) are challenging for existing approaches due to the complexity of objective functions and the massive volume of decision space. Some large-scale multiobjective evolutionary algorithms (LSMOEAs) have recently been proposed, which have shown their effectiveness in solving some benchmarks and real-world applications. They merely focus on handling the massive volume of decision space and ignore the complexity of LSMOPs in terms of objective functions. The complexity issue is also important since the complexity grows along with the increment in the number of decision variables. Our previous study proposed a framework to accelerate evolutionary large-scale multiobjective optimization via problem reformulation for handling large-scale decision variables. Here, we investigate the effectiveness of LSMOF combined with decomposition-based MOEA (MOEA/D), aiming to handle the complexity of LSMOPs in both the decision and objective spaces. Specifically, MOEA/D is embedded in LSMOF via two different strategies, and the proposed algorithm is tested on various benchmark LSMOPs. Experimental results indicate the encouraging performance improvement benefited from the solution of the complexity issue in large-scale multiobjective optimization.},
	booktitle = {2021 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Li, Lianghao and He, Cheng and Cheng, Ran and Pan, Linqiang},
	month = jun,
	year = {2021},
	keywords = {Benchmark testing, Complexity theory, Evolutionary computation, large-scale optimization, Linear programming, Multiobjective optimization, offspring generation, Optimization, problem decomposition, problem reformulation, Sociology, Statistics},
	pages = {2149--2155},
}

@article{lin_adaptive_2021,
	title = {Adaptive dropout for high-dimensional expensive multiobjective optimization},
	issn = {2198-6053},
	url = {https://doi.org/10.1007/s40747-021-00362-5},
	doi = {10.1007/s40747-021-00362-5},
	abstract = {Various works have been proposed to solve expensive multiobjective optimization problems (EMOPs) using surrogate-assisted evolutionary algorithms (SAEAs) in recent decades. However, most existing methods focus on EMOPs with less than 30 decision variables, since a large number of training samples are required to build an accurate surrogate model for high-dimensional EMOPs, which is unrealistic for expensive multiobjective optimization. To address this issue, we propose an SAEA with an adaptive dropout mechanism. Specifically, this mechanism takes advantage of the statistical differences between different solution sets in the decision space to guide the selection of some crucial decision variables. A new infill criterion is then proposed to optimize the selected decision variables with the assistance of surrogate models. Moreover, the optimized decision variables are extended to new full-length solutions, and then the new candidate solutions are evaluated using expensive functions to update the archive. The proposed algorithm is tested on different benchmark problems with up to 200 decision variables compared to some state-of-the-art SAEAs. The experimental results have demonstrated the promising performance and computational efficiency of the proposed algorithm in high-dimensional expensive multiobjective optimization.},
	language = {en},
	urldate = {2021-08-26},
	journal = {Complex \& Intelligent Systems},
	author = {Lin, Jianqing and He, Cheng and Cheng, Ran},
	month = apr,
	year = {2021},
	file = {Springer Full Text PDF:C\:\\Users\\PDELL\\Zotero\\storage\\YWWTV3E3\\Lin 等。 - 2021 - Adaptive dropout for high-dimensional expensive mu.pdf:application/pdf},
}


@article{wang_inverse_2021,
	title = {An inverse design method for supercritical airfoil based on conditional generative models},
	issn = {1000-9361},
	url = {https://www.sciencedirect.com/science/article/pii/S1000936121000662},
	doi = {10.1016/j.cja.2021.03.006},
	abstract = {Inverse design has long been an efficient and powerful design tool in the aircraft industry. In this paper, a novel inverse design method for supercritical airfoils is proposed based on generative models in deep learning. A Conditional Variational AutoEncoder (CVAE) and an integrated generative network CVAE-GAN that combines the CVAE with the Wasserstein Generative Adversarial Networks (WGAN), are conducted as generative models. They are used to generate target wall Mach distributions for the inverse design that matches specified features, such as locations of suction peak, shock and aft loading. Qualitative and quantitative results show that both adopted generative models can generate diverse and realistic wall Mach number distributions satisfying the given features. The CVAE-GAN model outperforms the CVAE model and achieves better reconstruction accuracies for all the samples in the dataset. Furthermore, a deep neural network for nonlinear mapping is adopted to obtain the airfoil shape corresponding to the target wall Mach number distribution. The performances of the designed deep neural network are fully demonstrated and a smoothness measurement is proposed to quantify small oscillations in the airfoil surface, proving the authenticity and accuracy of the generated airfoil shapes.},
	language = {en},
	urldate = {2021-08-01},
	journal = {Chinese Journal of Aeronautics},
	author = {Wang, Jing and Li, Runze and He, Cheng and Chen, Haixin and Cheng, Ran and Zhai, Chen and Zhang, Miao},
	year = {2021},
	keywords = {Conditional Variational AutoEncoder (CVAE), Deep learning, Generative Adversarial Networks (GAN), Generative models, Inverse design, Supercritical airfoil},
}

@article{yazdani_survey_2021,
	title = {A {Survey} of {Evolutionary} {Continuous} {Dynamic} {Optimization} {Over} {Two} {Decades}—{Part} {B}},
	volume = {25},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2021.3060012},
	abstract = {This article presents the second Part of a two-Part survey that reviews evolutionary dynamic optimization (EDO) for single-objective unconstrained continuous problems over the last two decades. While in the first part, we reviewed the components of dynamic optimization algorithms (DOAs); in this part, we present an in-depth review of the most commonly used benchmark problems, performance analysis methods, static optimization methods used in the framework of DOAs, and real-world applications. Compared to the previous works, this article provides a new taxonomy for the benchmark problems used in the field based on their baseline functions and dynamics. In addition, this survey classifies the commonly used performance indicators into fitness/error-based and efficiency-based ones. Different types of plots used in the literature for analyzing the performance and behavior of algorithms are also reviewed. Furthermore, the static optimization algorithms that are modified and utilized in the framework of DOAs as the optimization components are covered. We then comprehensively review some real-world dynamic problems that are optimized by EDO methods. Finally, some challenges and opportunities are pointed out for future directions.},
	number = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Yazdani, Danial and Cheng, Ran and Yazdani, Donya and Branke, Jürgen and Jin, Yaochu and Yao, Xin},
	year = {2021},
	keywords = {Optimization, Benchmark testing, Computer science, Continuous dynamic real-world problems, dynamic benchmark problems, evolutionary algorithms, future directions, Generators, Heuristic algorithms, Optimization methods, Performance analysis, performance indicators, unconstrained continuous dynamic optimization},
	pages = {630--650},
	annote = {Conference Name: IEEE Transactions on Evolutionary Computation},
}

@article{yazdani_adaptive_2020,
	title = {Adaptive {Control} of {Subpopulations} in {Evolutionary} {Dynamic} {Optimization}},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2020.3036100},
	abstract = {Multipopulation methods are highly effective in solving dynamic optimization problems. Three factors affect this significantly: 1) the exclusion mechanisms to avoid the convergence to the same peak by multiple subpopulations; 2) the resource allocation mechanism that assigns the computational resources to the subpopulations; and 3) the control mechanisms to adaptively adjust the number of subpopulations by considering the number of optima and available computational resources. In the existing exclusion mechanisms, when the distance (i.e., the distance between their best found positions) between two subpopulations becomes less than a predefined threshold, the inferior one will be removed/reinitialized. However, this leads to incapability of algorithms in covering peaks/optima that are closer than the threshold. Moreover, despite the importance of resource allocation due to the limited available computational resources between environmental changes, it has not been well studied in the literature. Finally, the number of subpopulations should be adapted to the number of optima. However, in most existing adaptive multipopulation methods, there is no predefined upper bound for generating subpopulations. Consequently, in problems with large numbers of peaks, they can generate too many subpopulations sharing limited computational resources. In this article, a multipopulation framework is proposed to address the aforementioned issues by using three adaptive approaches: 1) subpopulation generation; 2) double-layer exclusion; and 3) computational resource allocation. The experimental results demonstrate the superiority of the proposed framework over several peer approaches in solving various benchmark problems.},
	journal = {IEEE Transactions on Cybernetics},
	author = {Yazdani, Danial and Cheng, Ran and He, Cheng and Branke, Jürgen},
	year = {2020},
	keywords = {Sociology, Statistics, Computational resource allocation, dynamic optimization problems (DOPs), Euclidean distance, multipopulation, Resource management, Round robin, Tracking, tracking moving optima (TMO), Upper bound},
	pages = {1--14},
	annote = {Conference Name: IEEE Transactions on Cybernetics},
}

@article{yazdani_benchmarking_2020,
	title = {Benchmarking {Continuous} {Dynamic} {Optimization}: {Survey} and {Generalized} {Test} {Suite}},
	issn = {2168-2275},
	shorttitle = {Benchmarking {Continuous} {Dynamic} {Optimization}},
	doi = {10.1109/TCYB.2020.3011828},
	abstract = {Dynamic changes are an important and inescapable aspect of many real-world optimization problems. Designing algorithms to find and track desirable solutions while facing challenges of dynamic optimization problems is an active research topic in the field of swarm and evolutionary computation. To evaluate and compare the performance of algorithms, it is imperative to use a suitable benchmark that generates problem instances with different controllable characteristics. In this article, we give a comprehensive review of existing benchmarks and investigate their shortcomings in capturing different problem features. We then propose a highly configurable benchmark suite, the generalized moving peaks benchmark, capable of generating problem instances whose components have a variety of properties, such as different levels of ill-conditioning, variable interactions, shape, and complexity. Moreover, components generated by the proposed benchmark can be highly dynamic with respect to the gradients, heights, optimum locations, condition numbers, shapes, complexities, and variable interactions. Finally, several well-known optimizers and dynamic optimization algorithms are chosen to solve generated problems by the proposed benchmark. The experimental results show the poor performance of the existing methods in facing new challenges posed by the addition of new properties.},
	journal = {IEEE Transactions on Cybernetics},
	author = {Yazdani, Danial and Omidvar, Mohammad Nabi and Cheng, Ran and Branke, Jürgen and Nguyen, Trung Thanh and Yao, Xin},
	year = {2020},
	keywords = {Optimization, Benchmark testing, Linear programming, Shape, Generators, Heuristic algorithms, dynamic optimization problems (DOPs), Complexity theory, Dynamic environments, evolutionary dynamic optimization, moving peaks benchmark (MPB), survey, tracking moving optimum (TMO)},
	pages = {1--14},
	annote = {Conference Name: IEEE Transactions on Cybernetics},
}


@article{kong_constructing_2020,
	title = {Constructing an automatic diagnosis and severity-classification model for acromegaly using facial photographs by deep learning},
	volume = {13},
	issn = {1756-8722},
	url = {https://doi.org/10.1186/s13045-020-00925-y},
	doi = {10.1186/s13045-020-00925-y},
	abstract = {Due to acromegaly’s insidious onset and slow progression, its diagnosis is usually delayed, thus causing severe complications and treatment difficulty. A convenient screening method is imperative. Based on our previous work, we herein developed a new automatic diagnosis and severity-classification model for acromegaly using facial photographs by deep learning on the data of 2148 photographs at different severity levels. Each photograph was given a score reflecting its severity (range 1{\textasciitilde}3). Our developed model achieved a prediction accuracy of 90.7\% on the internal test dataset and outperformed the performance of ten junior internal medicine physicians (89.0\%). The prospect of applying this model to real clinical practices is promising due to its potential health economic benefits.},
	number = {1},
	urldate = {2021-08-26},
	journal = {Journal of Hematology \& Oncology},
	author = {Kong, Yanguo and Kong, Xiangyi and He, Cheng and Liu, Changsong and Wang, Liting and Su, Lijuan and Gao, Jun and Guo, Qi and Cheng, Ran},
	month = jul,
	year = {2020},
	keywords = {Acromegaly, Deep learning, Facial photographs, Severity-classification model},
	pages = {88},
	file = {Full Text PDF:C\:\\Users\\PDELL\\Zotero\\storage\\CQ747D68\\Kong 等。 - 2020 - Constructing an automatic diagnosis and severity-c.pdf:application/pdf;Snapshot:C\:\\Users\\PDELL\\Zotero\\storage\\WSIUX8TE\\s13045-020-00925-y.html:text/html},
}


@article{he_evolutionary_2020,
	title = {Evolutionary {Large}-{Scale} {Multiobjective} {Optimization} for {Ratio} {Error} {Estimation} of {Voltage} {Transformers}},
	volume = {24},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2020.2967501},
	abstract = {Ratio error (RE) estimation of the voltage transformers (VTs) plays an important role in modern power delivery systems. Existing RE estimation methods mainly focus on periodical calibration but ignore the time-varying property. Consequently, it is difficult to efficiently estimate the state of the VTs in real time. To address this issue, we formulate a time-varying RE estimation (TREE) problem into a large-scale multiobjective optimization problem, where the multiple objectives and inequality constraints are formulated by statistical and physical rules extracted from the power delivery systems. Furthermore, a set of TREE problems from different substations is systematically formulated into a benchmark test suite for characterizing their different properties. The formulation of these TREE problems not only transfers an expensive RE estimation task to a relatively cheaper optimization problem but also promotes the research in large-scale multiobjective optimization by providing a real-world benchmark test suite with complex variable interactions and correlations to different objectives. To the best of our knowledge, this is the first time to formulate a real-world problem into a benchmark test suite for large-scale multiobjective optimization, and it is also the first work proposing to solve TREE problems via evolutionary multiobjective optimization.},
	number = {5},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {He, Cheng and Cheng, Ran and Zhang, Chuanji and Tian, Ye and Chen, Qin and Yao, Xin},
	year = {2020},
	keywords = {Optimization, Benchmark testing, Estimation, Benchmark test suite, Calibration, Error analysis, inequality constraint, large-scale multiobjective optimization, Substations, time-varying ratio error estimation (TREE), Voltage measurement, voltage transformer (VT)},
	pages = {868--881},
	annote = {Conference Name: IEEE Transactions on Evolutionary Computation},
}

@article{hou_reformulating_2020,
	title = {Reformulating preferences into constraints for evolutionary multi- and many-objective optimization},
	volume = {541},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520305223},
	doi = {10.1016/j.ins.2020.05.103},
	abstract = {Despite that the reference point based preference articulation plays a vital role in evolutionary multi- and many-objective optimization, three issues remain challenging. First, the performance of reference point based preference articulation largely depends on the location of the reference point. Second, the parameter settings for controlling the region of interest are not robust to the Pareto optimal fronts with different complicated shapes. Third, most existing methods have poor scalability to the number of objectives. To meet these challenges, we propose to reformulate preferences into constraints for evolutionary multi- and many-objective optimization. Extensive experiments on a variety of benchmark problems are conducted to demonstrate the effectiveness of our proposed method.},
	language = {en},
	urldate = {2021-08-01},
	journal = {Information Sciences},
	author = {Hou, Zhanglu and He, Cheng and Cheng, Ran},
	year = {2020},
	keywords = {Multiobjective optimization, Constraint handling, Many-objective optimization, Preference articulation},
	pages = {1--15},
}

@article{he_adaptive_2020,
	title = {Adaptive {Offspring} {Generation} for {Evolutionary} {Large}-{Scale} {Multiobjective} {Optimization}},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2020.3003926},
	abstract = {Offspring generation plays an important role in evolutionary multiobjective optimization. However, generating promising candidate solutions effectively in high-dimensional spaces is particularly challenging. To address this issue, we propose an adaptive offspring generation method for large-scale multiobjective optimization. First, a preselection strategy is proposed to select a balanced parent population, and then these parent solutions are used to construct direction vectors in the decision spaces for reproducing promising offspring solutions. Specifically, two kinds of direction vectors are adaptively used to generate offspring solutions. The first kind takes advantage of the dominated solutions to generate offspring solutions toward the Pareto optimal set (PS) for convergence enhancement, while the other kind uses those nondominated solutions to spread the solutions over the PS for diversity maintenance. The proposed offspring generation method can be embedded in many existing multiobjective evolutionary algorithms (EAs) for large-scale multiobjective optimization. Experiments are conducted to reveal the mechanism of our proposed adaptive reproduction strategy and validate its effectiveness. Experimental results on some large-scale multiobjective optimization problems have demonstrated the competitive performance of our proposed algorithm in comparison with five state-of-the-art large-scale EAs.},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {He, Cheng and Cheng, Ran and Yazdani, Danial},
	year = {2020},
	keywords = {Convergence, Evolutionary computation, Sociology, Pareto optimization, multiobjective optimization, Maintenance engineering, evolutionary algorithm (EA), Adaptive offspring generation, large-scale},
	pages = {1--13},
	annote = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics: Systems},
}

@article{cheng_model-based_2018,
	title = {Model-based evolutionary algorithms: a short survey},
	volume = {4},
	issn = {2198-6053},
	shorttitle = {Model-based evolutionary algorithms},
	url = {https://doi.org/10.1007/s40747-018-0080-1},
	doi = {10.1007/s40747-018-0080-1},
	abstract = {The evolutionary algorithms (EAs) are a family of nature-inspired algorithms widely used for solving complex optimization problems. Since the operators (e.g. crossover, mutation, selection) in most traditional EAs are developed on the basis of fixed heuristic rules or strategies, they are unable to learn the structures or properties of the problems to be optimized. To equip the EAs with learning abilities, recently, various model-based evolutionary algorithms (MBEAs) have been proposed. This survey briefly reviews some representative MBEAs by considering three different motivations of using models. First, the most commonly seen motivation of using models is to estimate the distribution of the candidate solutions. Second, in evolutionary multi-objective optimization, one motivation of using models is to build the inverse models from the objective space to the decision space. Third, when solving computationally expensive problems, models can be used as surrogates of the fitness functions. Based on the review, some further discussions are also given.},
	language = {en},
	number = {4},
	urldate = {2021-08-01},
	journal = {Complex \& Intelligent Systems},
	author = {Cheng, Ran and He, Cheng and Jin, Yaochu and Yao, Xin},
	year = {2018},
	pages = {283--292},
}

@article{he_evolutionary_2021,
	title = {Evolutionary {Multiobjective} {Optimization} {Driven} by {Generative} {Adversarial} {Networks} ({GANs})},
	volume = {51},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2020.2985081},
	abstract = {Recently, increasing works have been proposed to drive evolutionary algorithms using machine-learning models. Usually, the performance of such model-based evolutionary algorithms is highly dependent on the training qualities of the adopted models. Since it usually requires a certain amount of data (i.e., the candidate solutions generated by the algorithms) for model training, the performance deteriorates rapidly with the increase of the problem scales due to the curse of dimensionality. To address this issue, we propose a multiobjective evolutionary algorithm driven by the generative adversarial networks (GANs). At each generation of the proposed algorithm, the parent solutions are first classified into real and fake samples to train the GANs; then the offspring solutions are sampled by the trained GANs. Thanks to the powerful generative ability of the GANs, our proposed algorithm is capable of generating promising offspring solutions in high-dimensional decision space with limited training data. The proposed algorithm is tested on ten benchmark problems with up to 200 decision variables. The experimental results on these test problems demonstrate the effectiveness of the proposed algorithm.},
	number = {6},
	journal = {IEEE Transactions on Cybernetics},
	author = {He, Cheng and Huang, Shihua and Cheng, Ran and Tan, Kay Chen and Jin, Yaochu},
	year = {2021},
	keywords = {Evolutionary computation, Optimization, Training data, Computational modeling, multiobjective optimization, Deep learning, evolutionary algorithm, Machine learning, Adaptation models, Generative adversarial networks, generative adversarial networks (GANs), machine learning},
	pages = {3129--3142},
	annote = {Conference Name: IEEE Transactions on Cybernetics},
}


@article{he_efficient_2021,
	title = {Efficient evolutionary neural architecture search by modular inheritable crossover},
	volume = {64},
	issn = {2210-6502},
	url = {https://www.sciencedirect.com/science/article/pii/S2210650221000559},
	doi = {10.1016/j.swevo.2021.100894},
	abstract = {Deep neural networks are widely used in the domain of image classification, and a large number of excellent deep neural networks have been proposed in recent years. However, hand-crafted neural networks often require human experts for elaborate designs, which can be time-consuming and error-prone. Hence, neural architecture search (NAS) methods have been proposed to design model architecture automatically. The evolutionary NAS methods have achieved encouraging results due to the global search capability of evolutionary algorithms. Nevertheless, most existing evolutionary NAS methods use only the mutation operator to generate offspring architectures. Consequently, the generated architectures could be pretty different from their parent architectures, failing to inherit the modular information to accelerate the convergence rate. We propose an efficient evolutionary method using a tailored crossover operator to address this deficiency, which enables the offspring architectures to inherit from their parent architectures. Moreover, we combine it with mutation operators under the framework of the evolutionary algorithm. Experimental results on both the CIFAR-10 and CIFAR-100 tasks show that our proposed evolutionary NAS method has achieved state-of-the-art results.},
	language = {en},
	urldate = {2021-08-26},
	journal = {Swarm and Evolutionary Computation},
	author = {He, Cheng and Tan, Hao and Huang, Shihua and Cheng, Ran},
	month = jul,
	year = {2021},
	keywords = {Evolutionary algorithm, Image classification, Inheritable crossover, Neural architecture search},
	pages = {100894},
}


@article{yazdani_survey_2021-1,
	title = {A {Survey} of {Evolutionary} {Continuous} {Dynamic} {Optimization} {Over} {Two} {Decades}—{Part} {A}},
	volume = {25},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2021.3060014},
	abstract = {Many real-world optimization problems are dynamic. The field of dynamic optimization deals with such problems where the search space changes over time. In this two-part article, we present a comprehensive survey of the research in evolutionary dynamic optimization for single-objective unconstrained continuous problems over the last two decades. In Part A of this survey, we propose a new taxonomy for the components of dynamic optimization algorithms (DOAs), namely, convergence detection, change detection, explicit archiving, diversity control, and population division and management. In comparison to the existing taxonomies, the proposed taxonomy covers some additional important components, such as convergence detection and computational resource allocation. Moreover, we significantly expand and improve the classifications of diversity control and multipopulation methods, which are underrepresented in the existing taxonomies. We then provide detailed technical descriptions and analysis of different components according to the suggested taxonomy. Part B of this survey provides an in-depth analysis of the most commonly used benchmark problems, performance analysis methods, static optimization algorithms used as the optimization components in the DOAs, and dynamic real-world applications. Finally, several opportunities for future work are pointed out.},
	number = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Yazdani, Danial and Cheng, Ran and Yazdani, Donya and Branke, Jürgen and Jin, Yaochu and Yao, Xin},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Evolutionary Computation},
	keywords = {Benchmark testing, Change detection, Classification algorithms, evolutionary algorithms (EA), Heuristic algorithms, multipopulation, Resource management, response component, Search problems, Sociology, taxonomy, Taxonomy, unconstrained continuous dynamic optimization},
	pages = {609--629},
}

@article{huang_fapn_2021,
	title = {{FaPN}: {Feature}-aligned {Pyramid} {Network} for {Dense} {Image} {Prediction}},
	shorttitle = {{FaPN}},
	url = {http://arxiv.org/abs/2108.07058},
	abstract = {Recent advancements in deep neural networks have made remarkable leap-forwards in dense image prediction. However, the issue of feature alignment remains as neglected by most existing approaches for simplicity. Direct pixel addition between upsampled and local features leads to feature maps with misaligned contexts that, in turn, translate to mis-classifications in prediction, especially on object boundaries. In this paper, we propose a feature alignment module that learns transformation offsets of pixels to contextually align upsampled higher-level features; and another feature selection module to emphasize the lower-level features with rich spatial details. We then integrate these two modules in a top-down pyramidal architecture and present the Feature-aligned Pyramid Network (FaPN). Extensive experimental evaluations on four dense prediction tasks and four datasets have demonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6 points in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In particular, our FaPN achieves the state-of-the-art of 56.7\% mIoU on ADE20K when integrated within Mask-Former. The code is available from https://github.com/EMI-Group/FaPN.},
	urldate = {2021-08-26},
	journal = {arXiv:2108.07058 [cs]},
	author = {Huang, Shihua and Lu, Zhichao and Cheng, Ran and He, Cheng},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.07058},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV2021},
}

@article{wang_end--end_2021,
	title = {End-to-{End} {Dense} {Video} {Captioning} with {Parallel} {Decoding}},
	url = {http://arxiv.org/abs/2108.07781},
	abstract = {Dense video captioning aims to generate multiple associated captions with their temporal locations from the video. Previous methods follow a sophisticated "localize-then-describe" scheme, which heavily relies on numerous hand-crafted components. In this paper, we proposed a simple yet effective framework for end-to-end dense video captioning with parallel decoding (PDVC), by formulating the dense caption generation as a set prediction task. In practice, through stacking a newly proposed event counter on the top of a transformer decoder, the PDVC precisely segments the video into a number of event pieces under the holistic understanding of the video content, which effectively increases the coherence and readability of predicted captions. Compared with prior arts, the PDVC has several appealing advantages: (1) Without relying on heuristic non-maximum suppression or a recurrent event sequence selection network to remove redundancy, PDVC directly produces an event set with an appropriate size; (2) In contrast to adopting the two-stage scheme, we feed the enhanced representations of event queries into the localization head and caption head in parallel, making these two sub-tasks deeply interrelated and mutually promoted through the optimization; (3) Without bells and whistles, extensive experiments on ActivityNet Captions and YouCook2 show that PDVC is capable of producing high-quality captioning results, surpassing the state-of-the-art two-stage methods when its localization accuracy is on par with them. Code is available at https://github.com/ttengwang/PDVC.},
	urldate = {2021-08-26},
	journal = {arXiv:2108.07781 [cs]},
	author = {Wang, Teng and Zhang, Ruimao and Lu, Zhichao and Zheng, Feng and Cheng, Ran and Luo, Ping},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.07781},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, I.4.9, I.5.4},
	annote = {Comment: Accepted by ICCV 2021},
}

@article{tan_relativenas_2021,
	title = {{RelativeNAS}: {Relative} {Neural} {Architecture} {Search} via {Slow}-{Fast} {Learning}},
	issn = {2162-2388},
	shorttitle = {{RelativeNAS}},
	doi = {10.1109/TNNLS.2021.3096658},
	abstract = {Despite the remarkable successes of convolutional neural networks (CNNs) in computer vision, it is time-consuming and error-prone to manually design a CNN. Among various neural architecture search (NAS) methods that are motivated to automate designs of high-performance CNNs, the differentiable NAS and population-based NAS are attracting increasing interests due to their unique characters. To benefit from the merits while overcoming the deficiencies of both, this work proposes a novel NAS method, RelativeNAS. As the key to efficient search, RelativeNAS performs joint learning between fast learners (i.e., decoded networks with relatively lower loss value) and slow learners in a pairwise manner. Moreover, since RelativeNAS only requires low-fidelity performance estimation to distinguish each pair of fast learner and slow learner, it saves certain computation costs for training the candidate architectures. The proposed RelativeNAS brings several unique advantages: 1) it achieves state-of-the-art performances on ImageNet with top-1 error rate of 24.88\%, that is, outperforming DARTS and AmoebaNet-B by 1.82\% and 1.12\%, respectively; 2) it spends only 9 h with a single 1080Ti GPU to obtain the discovered cells, that is, 3.75x and 7875x faster than DARTS and AmoebaNet, respectively; and 3) it provides that the discovered cells obtained on CIFAR-10 can be directly transferred to object detection, semantic segmentation, and keypoint detection, yielding competitive results of 73.1\% mAP on PASCAL VOC, 78.7\% mIoU on Cityscapes, and 68.5\% AP on MSCOCO, respectively. The implementation of RelativeNAS is available at https://github.com/EMI-Group/RelativeNAS.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tan, Hao and Cheng, Ran and Huang, Shihua and He, Cheng and Qiu, Changxiao and Yang, Fan and Luo, Ping},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {AutoML, Computer architecture, convolutional neural network (CNN), Estimation, neural architecture search (NAS), Neural networks, Optimization, population-based search, Search problems, slow-fast learning., Sociology, Statistics},
	pages = {1--15},
}

@article{cheng_solving_2019,
	title = {Solving {Incremental} {Optimization} {Problems} via {Cooperative} {Coevolution}},
	volume = {23},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2018.2883599},
	abstract = {Engineering designs can involve multiple stages, where at each stage, the design models are incrementally modified and optimized. In contrast to traditional dynamic optimization problems, where the changes are caused by some objective factors, the changes in such incremental optimization problems (IOPs) are usually caused by the modifications made by the decision makers during the design process. While existing work in the literature is mainly focused on traditional dynamic optimization, little research has been dedicated to solving such IOPs. In this paper, we study how to adopt cooperative coevolution to efficiently solve a specific type of IOPs, namely, those with increasing decision variables. First, we present a benchmark function generator on the basis of some basic formulations of IOPs with increasing decision variables and exploitable modular structure. Then, we propose a contribution-based cooperative coevolutionary framework coupled with an incremental grouping method for dealing with them. On one hand, the benchmark function generator is capable of generating various benchmark functions with various characteristics. On the other hand, the proposed framework is promising in solving such problems in terms of both optimization accuracy and computational efficiency. In addition, the proposed method is further assessed using a real-world application, i.e., the design optimization of a stepped cantilever beam.},
	number = {5},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Cheng, Ran and Omidvar, Mohammad Nabi and Gandomi, Amir H. and Sendhoff, Bernhard and Menzel, Stefan and Yao, Xin},
	year = {2019},
	keywords = {Optimization, Benchmark testing, Computer science, Linear programming, Generators, Aerodynamics, Cooperative coevolution (CC), experience-based optimization, incremental optimization problem (IOP), Signal generators, variable grouping},
	pages = {762--775},
	annote = {Conference Name: IEEE Transactions on Evolutionary Computation},
}

@inproceedings{hu_multi-objective_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-objective {Neural} {Architecture} {Search} with {Almost} {No} {Training}},
	isbn = {978-3-030-72062-9},
	doi = {10.1007/978-3-030-72062-9_39},
	abstract = {In the recent past, neural architecture search (NAS) has attracted increasing attention from both academia and industries. Despite the steady stream of impressive empirical results, most existing NAS algorithms are computationally prohibitive to execute due to the costly iterations of stochastic gradient descent (SGD) training. In this work, we propose an effective alternative, dubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of network architectures. By just training the last linear classification layer, RWE reduces the computational cost of evaluating an architecture from hours to seconds. When integrated within an evolutionary multi-objective algorithm, RWE obtains a set of efficient architectures with state-of-the-art performance on CIFAR-10 with less than two hours’ searching on a single GPU card. Ablation studies on rank-order correlations and transfer learning experiments to ImageNet have further validated the effectiveness of RWE.},
	language = {en},
	booktitle = {Evolutionary {Multi}-{Criterion} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Hu, Shengran and Cheng, Ran and He, Cheng and Lu, Zhichao},
	editor = {Ishibuchi, Hisao and Zhang, Qingfu and Cheng, Ran and Li, Ke and Li, Hui and Wang, Handing and Zhou, Aimin},
	year = {2021},
	keywords = {Neural architecture search, Evolutionary algorithms, Multi-objective optimization, Performance estimation},
	pages = {492--503},
}

@inproceedings{he_population_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Population {Sizing} of {Evolutionary} {Large}-{Scale} {Multiobjective} {Optimization}},
	isbn = {978-3-030-72062-9},
	doi = {10.1007/978-3-030-72062-9_4},
	abstract = {Large-scale multiobjective optimization problems (LSMOPs) are emerging and widely existed in real-world applications, which involve a large number of decision variables and multiple conflicting objectives. Evolutionary algorithms (EAs) are naturally suitable for multiobjective optimization due to their population-based property, allowing the search of optima simultaneously. Nevertheless, LSMOPs are challenging for conventional EAs, mainly due to the huge volume of search space in LSMOPs. Thus, it is important to explore the impact of the population sizing on the performance of conventional multiobjective EAs (MOEAs) in solving LSMOPs. In this work, we compare several representative MOEAs with different settings of population sizes on some transformer ratio error estimation (TREE) problems in the power system. These test cases are defined on combinations of three population sizes, three TREE problems, and five MOEAs. Our results indicate that the performances of conventional MOEAs with different population sizes in solving LSMOPs are different. The impact of population sizing is most significant for differential evolution based and particle swarm based MOEAs.},
	language = {en},
	booktitle = {Evolutionary {Multi}-{Criterion} {Optimization}},
	publisher = {Springer International Publishing},
	author = {He, Cheng and Cheng, Ran},
	editor = {Ishibuchi, Hisao and Zhang, Qingfu and Cheng, Ran and Li, Ke and Li, Hui and Wang, Handing and Zhou, Aimin},
	year = {2021},
	keywords = {Multiobjective optimization, Large-scale optimization, Population size, Transformer ratio error estimation},
	pages = {41--52},
}


@inproceedings{li_manifold_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Manifold {Learning} {Inspired} {Mating} {Restriction} for {Evolutionary} {Constrained} {Multiobjective} {Optimization}},
	isbn = {978-3-030-72062-9},
	doi = {10.1007/978-3-030-72062-9_24},
	abstract = {Mating restriction strategies are capable of restricting the distribution of parent solutions for effective offspring generation in evolutionary algorithms (EAs). Studies have shown the importance of these strategies in improving the performance of EAs for multiobjective optimization. Our previous study proposed a specific manifold learning inspired mating restriction (MLMR) strategy. It has shown promising capability of solving multiobjective optimization problems (MOPs) with complicated Pareto set shapes. However, the effect of mating restriction strategies in solving constrained MOPs is yet to be well studied. Here, we investigate the effectiveness of MLMR for solving constrained MOPs. The MLMR strategy is embedded into some representative multiobjective EAs and tested on various benchmark constrained MOPs. Experimental results indicate the encouraging performance of MLMR in constrained multiobjective optimization.},
	language = {en},
	booktitle = {Evolutionary {Multi}-{Criterion} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Li, Lianghao and He, Cheng and Cheng, Ran and Pan, Linqiang},
	editor = {Ishibuchi, Hisao and Zhang, Qingfu and Cheng, Ran and Li, Ke and Li, Hui and Wang, Handing and Zhou, Aimin},
	year = {2021},
	keywords = {Constraint handling, Mating restriction, Mating selection, Multiobjective optimization},
	pages = {296--307},
}
