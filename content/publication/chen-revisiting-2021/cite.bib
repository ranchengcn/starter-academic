@article{chen_revisiting_2021,
 abstract = {As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and few-shot knowledge transfer across tasks.},
 annote = {Comment: Accepted to EMNLP 2021},
 author = {Chen, Yiming and Zhang, Yan and Zhang, Chen and Lee, Grandee and Cheng, Ran and Li, Haizhou},
 file = {arXiv Fulltext PDF:files/277/Chen 等。 - 2021 - Revisiting Self-Training for Few-Shot Learning of .pdf:application/pdf;arXiv.org Snapshot:files/278/2110.html:text/html},
 journal = {arXiv:2110.01256 [cs]},
 keywords = {Computer Science - Computation and Language},
 month = {October},
 note = {arXiv: 2110.01256},
 title = {Revisiting Self-Training for Few-Shot Learning of Language Model},
 url = {http://arxiv.org/abs/2110.01256},
 urldate = {2021-11-23},
 year = {2021}
}

