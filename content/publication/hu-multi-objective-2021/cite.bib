@inproceedings{hu_multi-objective_2021,
 abstract = {In the recent past, neural architecture search (NAS) has attracted increasing attention from both academia and industries. Despite the steady stream of impressive empirical results, most existing NAS algorithms are computationally prohibitive to execute due to the costly iterations of stochastic gradient descent (SGD) training. In this work, we propose an effective alternative, dubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of network architectures. By just training the last linear classification layer, RWE reduces the computational cost of evaluating an architecture from hours to seconds. When integrated within an evolutionary multi-objective algorithm, RWE obtains a set of efficient architectures with state-of-the-art performance on CIFAR-10 with less than two hoursâ€™ searching on a single GPU card. Ablation studies on rank-order correlations and transfer learning experiments to ImageNet have further validated the effectiveness of RWE.},
 address = {Cham},
 author = {Hu, Shengran and Cheng, Ran and He, Cheng and Lu, Zhichao},
 booktitle = {Evolutionary Multi-Criterion Optimization},
 doi = {10.1007/978-3-030-72062-9_39},
 editor = {Ishibuchi, Hisao and Zhang, Qingfu and Cheng, Ran and Li, Ke and Li, Hui and Wang, Handing and Zhou, Aimin},
 isbn = {978-3-030-72062-9},
 keywords = {Multi-objective optimization, Neural architecture search, Evolutionary algorithms, Performance estimation},
 language = {en},
 pages = {492--503},
 publisher = {Springer International Publishing},
 series = {Lecture Notes in Computer Science},
 title = {Multi-objective Neural Architecture Search with Almost No Training},
 year = {2021}
}

